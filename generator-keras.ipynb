{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791ebd2f-e168-4421-961a-51daa6b9f58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage import io, transform\n",
    "import pywt\n",
    "import os\n",
    "import cv2\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51659c3-78f1-4cfb-8959-285a30da94a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import (\n",
    "    Input,Dense,Reshape,Flatten,Dropout,Concatenate,Softmax)\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import UpSampling2D, Conv2D,Conv2DTranspose\n",
    "from tensorflow.keras.models import  Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow \n",
    "from tensorflow.keras.layers import Lambda, ReLU\n",
    "from tensorflow.keras.losses import logcosh\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b229b83-2d26-4c61-900a-c468307d3f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dumpman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f866e8-5f95-4707-87ec-7c894799d098",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" #this is to set the GPU to be used\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" #if we have multiple GPUs, we can use 0,1 as well. Comment if no GPU available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d07ee3b-6060-41a2-aaa5-713fbaeb79c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dumpman.undumper('list1.pkl') #list of file names for input\n",
    "y = dumpman.undumper('list2.pkl') #list of file names for Ground Truth\n",
    "\n",
    "X_train, X_rem, y_train, y_rem = train_test_split(X, y, test_size=0.05, random_state=42)\n",
    "\n",
    "\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_rem, y_rem, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01038969-254e-4252-a4a8-94da94d0d9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_train))\n",
    "print(len(X_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6c040c-d9a2-4cc8-87d3-42f08dd27806",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = './keras'\n",
    "\n",
    "batch = 4 #define the batch size. Higher batch size needs higher RAM as this amount of data needs to be on RAM every iteration\n",
    "num_epochs = 35 #this controls how many epochs to run the network for training, before stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5610ef5-c568-4333-b2dd-5e19a958c0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_row = 1024 #change these values to the size you need for your network\n",
    "img_col = 1024 #network size depends on this as well\n",
    "channels = 1\n",
    "img_shape = (img_row,img_col,channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd15b3cc-1698-4cf6-a688-5d68cf8ee02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c196a551-7a7d-48fa-a941-27bea82c4ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_same(inputs,filters,kernel=3,bn=True,drop=True): #function that returns\n",
    "    d=Conv2D(filters=filters,kernel_size=kernel,strides=1,padding='same')(inputs)\n",
    "    d=ReLU()(d)\n",
    "    if bn:\n",
    "        d= BatchNormalization()(d)\n",
    "    if drop:\n",
    "        d = Dropout(0.5)(d)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341227be-36c5-43fc-8c86-68e1accf256d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_half(inputs,filters,kernel=3,bn=True,drop=True): #function that returns\n",
    "    d=Conv2D(filters=filters,kernel_size=kernel,strides=2,padding='same')(inputs)\n",
    "    d=ReLU()(d)\n",
    "    if bn:\n",
    "        d= BatchNormalization()(d)\n",
    "    if drop:\n",
    "        d = Dropout(0.5)(d)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbae8562-18da-4bca-a776-d182101b643d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generator(): #class definition for the network\n",
    "    \n",
    "    inputs=Input(shape=(img_row,img_col,channels))\n",
    "    \n",
    "    inp = encode_same(inputs,1,bn=False,drop=False)\n",
    "\n",
    "    e = encode_half(inp,3) #half the shape\n",
    "\n",
    "    op = Conv2D(3, (3, 3), strides =(1,1), padding=\"same\", activation = \"sigmoid\")(e) \n",
    "    \n",
    "    return Model(inputs,op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b425a779-e1ad-43d9-83d7-6a79b02c9195",
   "metadata": {},
   "outputs": [],
   "source": [
    "G=generator() #creates a variable for the DL network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fc30fb-3c82-46a9-acff-60ab1ab9e687",
   "metadata": {},
   "outputs": [],
   "source": [
    "G.summary() #displays the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da40c150-8908-4253-9674-5d8c49ced399",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=Adam(0.1) #create an optimizer. We use Adam\n",
    "\n",
    "img = Input(shape=img_shape) #define the input shape variable\n",
    "op = Input(shape=(img_row/2,img_col/2,channels)) #define the output shape variable\n",
    "\n",
    "G.compile(loss=['mse'], optimizer=optimizer) #compile our network with initialization parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7354899-7f11-452a-a018-7d7c29b238f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_sch(epoch, lr): #this will control the learning rate. Every 3 epochs, make the LR 10% of previous value\n",
    "    decay_rate = 0.1\n",
    "    decay_step = 3\n",
    "    if epoch % decay_step == 0 and epoch:\n",
    "        return lr * decay_rate\n",
    "    return lr\n",
    "    \n",
    "lr_scheduler = tensorflow.keras.callbacks.LearningRateScheduler(lr_sch) #integrate the LR function to work in Keras scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496c36f5-8d13-4074-b662-61dca06be23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_reducer = tensorflow.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, mode='min') #reduce the LR depending on validation loss\n",
    "checkpointer = tensorflow.keras.callbacks.ModelCheckpoint(filepath=fname, verbose=1, monitor='loss', mode='min',\n",
    "                                               save_weights_only=True, save_best_only=True) #save network based on training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a75f1b-2a44-4848-aa37-23a03de94623",
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_Generator(tensorflow.keras.utils.Sequence): #this will be our training generator, sub classed from tensorflow.keras.utils.Sequence\n",
    "\n",
    "    def __init__(self, input_filenames, output_filenames, batch_size= batch):\n",
    "        self.input_filenames, self.output_filenames = input_filenames, output_filenames #this copies the values to local variables for our immediate use\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def __len__(self):\n",
    "        return np.uint16(np.ceil(len(self.input_filenames) / float(self.batch_size))) #since the number of input and output files are same, we can use either\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.input_filenames[idx * self.batch_size:(idx + 1) * self.batch_size] #creates the batches of file names\n",
    "        batch_y = self.output_filenames[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        \n",
    "        inp = np.zeros((len(batch_x),img_row,img_col,1),dtype=np.float32) #we go from a 1 layer input, like a grayscale image        \n",
    "        op = np.zeros((len(batch_y),img_row/2,img_col/2,3),dtype=np.float32) #to a 3 layer output like RGB, for something like colorization\n",
    "            \n",
    "        for i in range(len(batch_x)): \n",
    "            inp[i,:,:,0] = cv2.resize(cv2.imread(batch_x[i], 0),(img_col,img_row)) #note that unlike PyTorch, the last dimension is 0\n",
    "            #here we read the file name in grayscale (0 flag in opencv), then resize it to our desired shape for DL\n",
    "            #and then copy the values into our blank input array\n",
    "            \n",
    "            op[i] = cv2.resize(cv2.imread(batch_y[i]),(img_col/2,img_row/2)) #we do not need to reshape unlike PyTorch\n",
    "            #we do the same thing here, but since the files are 3 layered, we resize and then reshape since \n",
    "            #opencv has the shape as col,row,3 while PyTorch uses 3,col,row\n",
    "        \n",
    "        return (inp/255, op/255) #normalize to 0-1 values for our use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7aa272-471a-4595-b731-3b9c0bc6315c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_val_Generator(tensorflow.keras.utils.Sequence): #validation generator with the same things. But different name\n",
    "    \n",
    "    def __init__(self, input_filenames, output_filenames, batch_size= batch):\n",
    "        self.input_filenames, self.output_filenames = input_filenames, output_filenames #this copies the values to local variables for our immediate use\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def __len__(self):\n",
    "        return np.uint16(np.ceil(len(self.input_filenames) / float(self.batch_size))) #since the number of input and output files are same, we can use either\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.input_filenames[idx * self.batch_size:(idx + 1) * self.batch_size] #creates the batches of file names\n",
    "        batch_y = self.output_filenames[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        \n",
    "        inp = np.zeros((len(batch_x),1,img_row,img_col),dtype=np.float32) #we go from a 1 layer input, like a grayscale image        \n",
    "        op = np.zeros((len(batch_y),3,img_row/2,img_col/2),dtype=np.float32) #to a 3 layer output like RGB, for something like colorization\n",
    "            \n",
    "        for i in range(len(batch_x)): \n",
    "            inp[i,:,:,0] = cv2.resize(cv2.imread(batch_x[i], 0),(img_col,img_row)),(img_col,img_row)\n",
    "            #here we read the file name in grayscale (0 flag in opencv), then resize it to our desired shape for DL\n",
    "            #and then copy the values into our blank input array\n",
    "            \n",
    "            op[i] = np.reshape(cv2.resize(cv2.imread(batch_y[i]),(img_col/2,img_row/2)),(3,img_col/2,img_row/2))\n",
    "            #we do the same thing here, but since the files are 3 layered, we resize and then reshape since \n",
    "            #opencv has the shape as col,row,3 while PyTorch uses 3,col,row\n",
    "        \n",
    "        return (inp/255, op/255) #normalize to 0-1 values for our use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521829a9-91f3-4a8b-b816-68080ed36b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_training_batch_generator = My_Generator(X_train, y_train, batch_size= batch) #create function for training generator\n",
    "my_validation_batch_generator = My_val_Generator(X_valid, y_valid, batch_size= batch) #create function for validation generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677049e3-5ec2-46f9-ba62-5f87356d6274",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = G.fit(my_training_batch_generator, steps_per_epoch=(len(X_train) // batch),                                          \n",
    "                                          epochs=num_epochs,\n",
    "                                          callbacks=[lr_reducer, lr_scheduler, checkpointer],\n",
    "                                          verbose=1,\n",
    "                                          validation_data=my_validation_batch_generator,\n",
    "                                          validation_steps=(len(X_valid) // batch)) \n",
    "#this will take care of the training, including saving, based on the different LR variables we have created beforehand\n",
    "\n",
    "# G.save_weights(fname1, overwrite=True) #we can use this at the end to save the last epoch, but rename it just in case it is not the best output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8500594-7499-4f7b-8f09-39d239b5bd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list = X_test #this is our training list\n",
    "\n",
    "G.load_weights(fname)  #load the trained weights                \n",
    "\n",
    "for i, img in enumerate(img_list): #go through each value in list\n",
    "    \n",
    "    name = img[46:len(img)-4] #get the name. Experiment and get the exact name to save against\n",
    "    # print(name)\n",
    "    \n",
    "    inp = cv2.imread(img, 0) #make the input image as grayscale\n",
    "    temp = np.array(inp)\n",
    "    \n",
    "    cv2.imwrite('./data/test/'+(name) + '_ip.png', inp) #write the image to disk\n",
    "    \n",
    "    shape0 = temp.shape[0] #calculate the shapes for reshaping later\n",
    "    shape1 = temp.shape[1]\n",
    "    \n",
    "    inp = cv2.resize(inp, (img_col, img_row))   #resize the input to our input size\n",
    "    \n",
    "    d = np.reshape(inp, (1, 1, img_col, img_row) #make it a 4D tensor\n",
    "    \n",
    "    d = d.astype('float') #go to float\n",
    "\n",
    "    k = G.predict(d) #run the model and get the output\n",
    "    \n",
    "    op = cv2.resize(k[0]*255, (shape0/2, shape1/2)) #resize the output to our actual size/2 after normalization back to 0-255\n",
    "    \n",
    "    cv2.imwrite('./data/test/'+(name) + '_op.jpg', op) #save the op. No need to reshape as Keras outputs as channels last. This is a channel last output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e80dc4-e72a-4028-a873-44b7b3583bd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
