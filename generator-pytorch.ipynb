{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8149d929-7ee1-47bb-9530-cb622bcecadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.init import kaiming_normal_, orthogonal_\n",
    "import torchvision.models as models\n",
    "from torch.nn.functional import threshold, normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f0c9f4-9451-4aa2-9ba8-5c8bab54ee72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09cde79-209d-4999-80f6-dc3a2d611bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" #this is to set the GPU to be used\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" #if we have multiple GPUs, we can use 0,1 as well. Comment if no GPU available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e7103d-a58e-45cf-9292-0adfc4689db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dumpman.undumper('list1.pkl') #list of file names for input\n",
    "y = dumpman.undumper('list2.pkl') #list of file names for Ground Truth\n",
    "\n",
    "X_train, X_rem, y_train, y_rem = train_test_split(X, y, test_size=0.05, random_state=42)\n",
    "\n",
    "\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_rem, y_rem, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355cc971-525e-4eed-b284-1ae2f8bb15b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_train))\n",
    "print(len(X_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08c01f3-8263-4ab3-92c5-bcc5e3649c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = './pytorch'\n",
    "\n",
    "batch = 4\n",
    "num_epochs = 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a4f3c1-cdef-470d-aa75-4b1aaa3ac988",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_row = 1024 #change these values to the size you need for your network\n",
    "img_col = 1024 #network size depends on this as well\n",
    "channels = 1\n",
    "img_shape = (img_row,img_col,channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f515616-f55a-4c8d-901d-56a502deca70",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89afb1a5-dcc6-4d66-8b7e-748808fded1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_half(filters_in,filters_out,bn=True,drop=True): #an example of how to create functions for use with the DL Network you will use\n",
    "    if bn and drop:\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(filters_in, filters_out, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.ReLU(0.1),\n",
    "            nn.BatchNorm2d(filters_out),            \n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "    if (bn==True and drop==False):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(filters_in, filters_out, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.ReLU(0.1),\n",
    "            nn.BatchNorm2d(filters_out)\n",
    "        )\n",
    "    if (bn==False and drop==True):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(filters_in, filters_out, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.ReLU(0.1),           \n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "    if (bn==False and drop==False):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(filters_in, filters_out, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.ReLU(0.1)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9573a9c2-f003-4111-ad46-098b3732b1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepLearning(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepLearning,self).__init__()\n",
    "        \n",
    "        # CNN  \n",
    "        self.inp = nn.Conv2d(1, 1, kernel_size=3, stride=1, padding=1, bias=False) \n",
    "        \n",
    "        self.enc = encode_half(1,3) #input size is 1 and output is 3\n",
    "\n",
    "        self.op2 = nn.Conv2d(3, 3, kernel_size=3, stride=1, padding=1) #again 3 to 3\n",
    "        #sigmoid\n",
    "        self.sigmoid = nn.Sigmoid() #needed to normalize if you are working with images\n",
    "        \n",
    "    def forward(self, x, op1): \n",
    " \n",
    "        x = self.inp(x)        \n",
    "        x = self.enc(x)                \n",
    "        x = self.op(x)\n",
    "        op = self.sigmoid(x)        \n",
    "        \n",
    "        return op #returns the output  \n",
    "\n",
    "    def get_loss(self, input_value, output_value, opt):\n",
    "        \n",
    "        # define your loss funciton\n",
    "        loss = torch.nn.functional.mse_loss(output_value, input_value) #we use MSE here and show how to calculate the loss given an output and an input\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def step(self, inp, op, optimizer):\n",
    "        \n",
    "        optimizer.zero_grad() #this is needed to make optimizer zero for each epoch, so that the network learns new values\n",
    "        loss = self.get_loss(inp, op) #calls the loss funciton and calculates loss\n",
    "        loss.backward() #creates the backpropagation graph linking everything\n",
    "        optimizer.step() #creates a step function which would save the whole generated graph\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcac21c-8422-469f-92b3-b35adf840ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "M_DL = DeepLearning() #creates an instance of the Deep Learning model\n",
    "device = \"cpu\" #creates the default location\n",
    "\n",
    "if use_cuda: #depending on the flag, the location where the model resides is changed\n",
    "    print('CUDA used.')\n",
    "    M_DL = M_DL.to(\"cuda\")\n",
    "    device = \"cuda\"\n",
    "    \n",
    "optimizer = torch.optim.Adam(M_DL.parameters(), lr=0.1, betas=(0.9, 0.999)) #creates our optimizer with whatever default value we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd86c76d-4594-4705-8d0c-7c58d3dfa1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_Generator(torch.utils.data.Dataset): #this will be our training generator\n",
    "\n",
    "    def __init__(self, input_filenames, output_filenames, batch_size= batch):\n",
    "        self.input_filenames, self.output_filenames = input_filenames, output_filenames #this copies the values to local variables for our immediate use\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def __len__(self):\n",
    "        return np.uint16(np.ceil(len(self.input_filenames) / float(self.batch_size))) #since the number of input and output files are same, we can use either\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.input_filenames[idx * self.batch_size:(idx + 1) * self.batch_size] #creates the batches of file names\n",
    "        batch_y = self.output_filenames[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        \n",
    "        inp = np.zeros((len(batch_x),1,img_row,img_col),dtype=np.float32) #we go from a 1 layer input, like a grayscale image        \n",
    "        op = np.zeros((len(batch_y),3,img_row,img_col),dtype=np.float32) #to a 3 layer output like RGB, for something like colorization\n",
    "            \n",
    "        for i in range(len(batch_x)): \n",
    "            inp[i,:,:,0] = cv2.resize(cv2.imread(batch_x[i], 0),(img_col,img_row)),(img_col,img_row)\n",
    "            #here we read the file name in grayscale (0 flag in opencv), then resize it to our desired shape for DL\n",
    "            #and then copy the values into our blank input array\n",
    "            \n",
    "            op[i] = np.reshape(cv2.resize(cv2.imread(batch_z[i]),(img_col,img_row)),(3,img_col,img_row))\n",
    "            #we do the same thing here, but since the files are 3 layered, we resize and then reshape since \n",
    "            #opencv has the shape as col,row,3 while PyTorch uses 3,col,row\n",
    "        \n",
    "        return (inp/255, op/255) #normalize to 0-1 values for our use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dac51d-b374-4b0f-ab42-f7904e136c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_val_Generator(torch.utils.data.Dataset): #validation generator with the same things. but different name\n",
    "\n",
    "    def __init__(self, input_filenames, output_filenames, batch_size= batch):\n",
    "        self.input_filenames, self.output_filenames = input_filenames, output_filenames #this copies the values to local variables for our immediate use\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def __len__(self):\n",
    "        return np.uint16(np.ceil(len(self.input_filenames) / float(self.batch_size))) #since the number of input and output files are same, we can use either\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.input_filenames[idx * self.batch_size:(idx + 1) * self.batch_size] #creates the batches of file names\n",
    "        batch_y = self.output_filenames[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        \n",
    "        inp = np.zeros((len(batch_x),1,img_row,img_col),dtype=np.float32) #we go from a 1 layer input, like a grayscale image        \n",
    "        op = np.zeros((len(batch_y),3,img_row,img_col),dtype=np.float32) #to a 3 layer output like RGB, for something like colorization\n",
    "            \n",
    "        for i in range(len(batch_x)): \n",
    "            inp[i,:,:,0] = cv2.resize(cv2.imread(batch_x[i], 0),(img_col,img_row)),(img_col,img_row)\n",
    "            #here we read the file name in grayscale (0 flag in opencv), then resize it to our desired shape for DL\n",
    "            #and then copy the values into our blank input array\n",
    "            \n",
    "            op[i] = np.reshape(cv2.resize(cv2.imread(batch_z[i]),(img_col,img_row)),(3,img_col,img_row))\n",
    "            #we do the same thing here, but since the files are 3 layered, we resize and then reshape since \n",
    "            #opencv has the shape as col,row,3 while PyTorch uses 3,col,row\n",
    "        \n",
    "        return (inp/255, op/255) #normalize to 0-1 values for our use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c14faf7-d87c-4034-bb44-73043b5e0bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_training_batch_generator = My_Generator(X_train, y_trainbatch_size= batch)\n",
    "my_validation_batch_generator = My_val_Generator(X_valid, y_valid, batch_size= batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f04d2c8-caf8-4b7c-9c16-4a87dc02418c",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_loss_t = 1e10 #base value for training\n",
    "min_loss_v = 1e10 #base value for validation\n",
    "\n",
    "M_DL.train() #indicates that we are using the train module instead of validation\n",
    "\n",
    "for ep in range(num_epochs):\n",
    "    \n",
    "\tst_t = time.time() #get current time\n",
    "    \n",
    "\tprint('='*50) #create a separation line so that it is easier to notice\n",
    "    \n",
    "\t# Train\n",
    "\tM_DL.train() #configure for training the model\n",
    "\n",
    "\tloss_mean = 0 #we will add to this value, so it is zero\n",
    "\tloss_list = [] #cerate a blank list\n",
    "    \n",
    "\ti = 0\n",
    "\tfor x, y in my_training_batch_generator:\n",
    "        \n",
    "\t\tx = torch.from_numpy(x) #configure these to numpy \n",
    "\t\ty = torch.from_numpy(y)\n",
    "        \n",
    "\t\tif use_cuda:       \n",
    "\t\t\tx = x.cuda(non_blocking=True) #load into cuda if configured\n",
    "\t\t\ty = y.cuda(non_blocking=True)\n",
    "            \n",
    "\t\tls = M_DL.step(x, y, optimizer) #this line is to send the input and output to the step funciton in the model\n",
    "        \n",
    "\t\tif(math.isnan(float(ls))): #in case we go to undefined loss\n",
    "\t\t\tbreak\n",
    "            \n",
    "\t\tloss_list.append(float(ls)) #appending every loss value to our loss list\n",
    "\t\tloss_mean += float(ls) #adding the value\n",
    "        \n",
    "\t\tif(i%10==0): #we can comment this if the whole epoch is not taking very long\n",
    "\t\t\tprint(\"Loss at iteration \" + str(i+1) + \" is \" + str(float(ls)))\n",
    "\t\ti+=1 #comment this if we do not need iteration loss values displayed\n",
    "        \n",
    "\tprint('Training epoch takes {:.1f} sec'.format(time.time()-st_t)) #print current epoch time taken\n",
    "    \n",
    "\tloss_mean /= len(X_train) #calculate the mean value\n",
    "\n",
    "\t# Validation\n",
    "\tst_t = time.time() #get the starting time for validation\n",
    "    \n",
    "\tM_DL.eval() #define that nothing will be trained at this level\n",
    "    \n",
    "\tloss_mean_valid = 0 \n",
    "\tv_loss_list = []\n",
    "    \n",
    "\tfor v_x, v_y in my_validation_batch_generator:\n",
    "        \n",
    "\t\tv_x = torch.from_numpy(v_x) #same as training stage\n",
    "\t\tv_y = torch.from_numpy(v_y)\n",
    "        \n",
    "\t\tif use_cuda:\n",
    "\t\t\tv_x = v_x.cuda(non_blocking=True) #same as training stage\n",
    "\t\t\tv_y = v_y.cuda(non_blocking=True)        \n",
    "            \n",
    "\t\tv_ls = M_DL.get_loss(v_x, v_y).data.cpu().numpy()     #same as training stage \n",
    "        \n",
    "\t\tif(math.isnan(float(v_ls))): #same as training stage\n",
    "\t\t\tbreak\n",
    "            \n",
    "\t\tv_loss_list.append(float(v_ls)) #same as training stage\n",
    "\t\tloss_mean_valid += float(v_ls)\n",
    "        \n",
    "\tprint('Validation takes {:.1f} sec'.format(time.time()-st_t))\n",
    "\tloss_mean_valid /= len(X_valid) #same as training stage\n",
    "\n",
    "\n",
    "\tf = open(fname, 'a') #opens our file in append stage, so that we do not overwrite it\n",
    "\tf.write('Epoch {}\\ntrain loss mean: {}\\nvalid loss mean: {}\\n'.format(ep+1, loss_mean, loss_mean_valid)) #store the loss and validation mean\n",
    "\tprint('Epoch {}\\ntrain loss mean: {}\\nvalid loss mean: {}\\n'.format(ep+1, loss_mean, loss_mean_valid)) #display the same\n",
    "\n",
    "\t# Save model if the validation loss decreases\n",
    "\tcheck_interval = 1\n",
    "\tif loss_mean_valid < min_loss_v and ep % check_interval == 0: #this is to check per interval if the loss decreases and store it if it does\n",
    "        \n",
    "\t\tmin_loss_v = loss_mean_valid #change the minimum loss so that network compares against that value next epoch\n",
    "\t\tprint('Save model at ep {}, mean of valid loss: {}'.format(ep+1, loss_mean_valid))  # print the value before saving\n",
    "        \n",
    "\t\ttorch.save(M_DL.state_dict(), fname+'_model.valid') #save the model\n",
    "\t\ttorch.save(optimizer.state_dict(), fname+'_opt.valid') #save the optimizer\n",
    "        \n",
    "\t# save if the training loss decrease\n",
    "\tcheck_interval = 1\n",
    "\tif loss_mean < min_loss_t and ep % check_interval == 0:\n",
    "        \n",
    "\t\tmin_loss_t = loss_mean #same as the validation step\n",
    "\t\tprint('Save model at ep {}, mean of train loss: {}'.format(ep+1, loss_mean))\n",
    "        \n",
    "\t\ttorch.save(M_DL.state_dict(), fname+'_model.train') #same as the validation step\n",
    "\t\ttorch.save(optimizer.state_dict(), fname+'_opt.train')\n",
    "        \n",
    "\tf.close() #close the file, once written to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20da1f11-56b9-4463-8974-43477cb7d82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import save_image #this library is needed to save the output we generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a04e224-0c6c-4cdb-abe2-426155a45e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "M_DL.eval() #tells network that we will just evaluate, and not to save anything\n",
    "for img in X_test:\n",
    "    \n",
    "\t\tinp = np.zeros((1,1,img_row,img_col),dtype=np.float32) #batch of 1 with 4 diemsional tensor since that is what the model expects\n",
    "\n",
    "\t\tname = img[46:len(img)-4] #get the name. Experiment and get the exact name to save against\n",
    "\t\t# print(name)\n",
    "        \n",
    "\t\tip = cv2.resize(cv2.imread(img, 0),(img_col,img_row)) #reshape to our needed input size\n",
    "\t\tcv2.imwrite('./data/test/'+(name) + '_ip.jpg', ip) #save the ip\n",
    "        \n",
    "\t\tinp[0,0] = ip/255 #we normalize input to our needed shape and store in our array\n",
    "\n",
    "\t\tinp = torch.from_numpy(inp) #convert to numpy\n",
    "        \n",
    "\t\tif use_cuda: #if using GPU\n",
    "\t\t\tinp = inp.cuda(non_blocking=True)\n",
    "            \n",
    "\t\toutput = M_DL(inp) #this will give us the normalized output\n",
    "\t\toutput = output.data.cpu().numpy() #bring it back to CPU for writing to our disk and convert to numpy \n",
    "\n",
    "\t\tcv2.imwrite('./data/test/'+(name) + '_op.jpg', output[0]*255) #save the output after normalizing back to 0-255 range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ba640c-2866-4389-9c54-943ef3b364dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
